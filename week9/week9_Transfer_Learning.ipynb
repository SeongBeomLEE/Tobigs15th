{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "week9_Transfer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c82580168b634ba08ba5f2a541d1f611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0732dc6e1ff84e08a98791647ea17fd1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bc2467c41c5747f390de9276ccf72f0e",
              "IPY_MODEL_58c3ba4fab384101a2ef5075249194ab"
            ]
          }
        },
        "0732dc6e1ff84e08a98791647ea17fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc2467c41c5747f390de9276ccf72f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f151e33bd5d44613a77ac8a7988e3bc1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33e3fd9ef6bc40fda1668a8c9c7e3f1f"
          }
        },
        "58c3ba4fab384101a2ef5075249194ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c37c724f97bf4da6a0babc5f6069b9ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:03&lt;00:00, 32.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1e0854929524dc0963a5bf9ea72d4e3"
          }
        },
        "f151e33bd5d44613a77ac8a7988e3bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33e3fd9ef6bc40fda1668a8c9c7e3f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c37c724f97bf4da6a0babc5f6069b9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1e0854929524dc0963a5bf9ea72d4e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytaWF9tLFQ0T"
      },
      "source": [
        "# Week 9 과제 1 - Tobigs 15기 이성범"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btozx60gSkCX"
      },
      "source": [
        "**잎 사진을 통한 질병 분류**\n",
        "\n",
        "*   데이터: 21개의 클래스로 구분된 약 20000장의 이미지\n",
        "><img src=\"https://drive.google.com/uc?id=1YQkxnNy61Gyi3Gp6ylCKeS72BVruJXr_\" width=\"700\" height=\"500\"> \n",
        "*   데이터 전처리\n",
        "    *   전체 데이터를 train,validation,test로 분할해주세요\n",
        "    *   저는 6:2:2로 분할했는데, 비율은 바꾸셔도 무방합니다.\n",
        "   \n",
        "    \n",
        "*   학습 진행 방향\n",
        "    *   Baseline 모델(pre-trained model 사용X) 구축\n",
        "    *   Pre-trained 모델 사용\n",
        "    *   Baseline과 ResNet50 모델의 성능을 비교해주세요 ~!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSXsUPLeJYmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb68cc62-9477-490b-ed40-afafc48e210c"
      },
      "source": [
        "# gdrive에 mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# 경로 설정\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f1MIh_ZEebi"
      },
      "source": [
        "# 파이썬으로 알집 푸는 법\n",
        "# !unzip -qq \"./Tobigs/tobigs_week9_plant_leaf_data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IchWiK1ZFQ0X"
      },
      "source": [
        "## 0. 데이터 분할"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jEtmXkaFQ0Z"
      },
      "source": [
        "* 데이터 분할을 위한 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fG-dk2dFQ0Z"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = './Tobigs/tobigs_week9_plant_leaf'\n",
        "classes_list = os.listdir(original_dataset_dir)\n",
        "\n",
        "base_dir = './splitted'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for cls in classes_list:\n",
        "    os.mkdir(os.path.join(train_dir, cls))\n",
        "    os.mkdir(os.path.join(validation_dir, cls))\n",
        "    os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oQyurQ6FQ0a"
      },
      "source": [
        "* 데이터 분할과 클래스별 데이터 수 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nrkw0nFQ0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df12a9f7-ea36-4864-b5d9-82497a37282d"
      },
      "source": [
        "import math\n",
        "\n",
        "for cls in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, cls)\n",
        "    fnames = os.listdir(path)\n",
        " \n",
        "    train_size = math.floor(len(fnames) * 0.6)\n",
        "    validation_size = math.floor(len(fnames) * 0.2)\n",
        "    test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "    train_fnames = fnames[:train_size]\n",
        "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "    for fname in test_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "        shutil.copyfile(src, dst)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size( Corn___Common_rust ):  715\n",
            "Validation size( Corn___Common_rust ):  238\n",
            "Test size( Corn___Common_rust ):  238\n",
            "Train size( Cherry___healthy ):  512\n",
            "Validation size( Cherry___healthy ):  170\n",
            "Test size( Cherry___healthy ):  170\n",
            "Train size( Apple___Black_rot ):  372\n",
            "Validation size( Apple___Black_rot ):  124\n",
            "Test size( Apple___Black_rot ):  124\n",
            "Train size( Apple___Apple_scab ):  378\n",
            "Validation size( Apple___Apple_scab ):  126\n",
            "Test size( Apple___Apple_scab ):  126\n",
            "Train size( Apple___healthy ):  987\n",
            "Validation size( Apple___healthy ):  329\n",
            "Test size( Apple___healthy ):  329\n",
            "Train size( Corn___healthy ):  697\n",
            "Validation size( Corn___healthy ):  232\n",
            "Test size( Corn___healthy ):  232\n",
            "Train size( Apple___Cedar_apple_rust ):  165\n",
            "Validation size( Apple___Cedar_apple_rust ):  55\n",
            "Test size( Apple___Cedar_apple_rust ):  55\n",
            "Train size( Cherry___Powdery_mildew ):  631\n",
            "Validation size( Cherry___Powdery_mildew ):  210\n",
            "Test size( Cherry___Powdery_mildew ):  210\n",
            "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
            "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
            "Train size( Corn___Northern_Leaf_Blight ):  591\n",
            "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
            "Test size( Corn___Northern_Leaf_Blight ):  197\n",
            "Train size( Pepper,_bell___Bacterial_spot ):  598\n",
            "Validation size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Test size( Pepper,_bell___Bacterial_spot ):  199\n",
            "Train size( Potato___healthy ):  91\n",
            "Validation size( Potato___healthy ):  30\n",
            "Test size( Potato___healthy ):  30\n",
            "Train size( Pepper,_bell___healthy ):  886\n",
            "Validation size( Pepper,_bell___healthy ):  295\n",
            "Test size( Pepper,_bell___healthy ):  295\n",
            "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
            "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
            "Train size( Grape___Esca_(Black_Measles) ):  829\n",
            "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
            "Test size( Grape___Esca_(Black_Measles) ):  276\n",
            "Train size( Strawberry___healthy ):  273\n",
            "Validation size( Strawberry___healthy ):  91\n",
            "Test size( Strawberry___healthy ):  91\n",
            "Train size( Potato___Early_blight ):  600\n",
            "Validation size( Potato___Early_blight ):  200\n",
            "Test size( Potato___Early_blight ):  200\n",
            "Train size( Grape___Black_rot ):  708\n",
            "Validation size( Grape___Black_rot ):  236\n",
            "Test size( Grape___Black_rot ):  236\n",
            "Train size( Grape___healthy ):  251\n",
            "Validation size( Grape___healthy ):  83\n",
            "Test size( Grape___healthy ):  83\n",
            "Train size( Potato___Late_blight ):  600\n",
            "Validation size( Potato___Late_blight ):  200\n",
            "Test size( Potato___Late_blight ):  200\n",
            "Train size( Strawberry___Leaf_scorch ):  665\n",
            "Validation size( Strawberry___Leaf_scorch ):  221\n",
            "Test size( Strawberry___Leaf_scorch ):  221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZDnksn0FQ0c"
      },
      "source": [
        "## 1. 베이스라인 모델을 구축해 주세요\n",
        "* Pre-Trained Model을 사용하지 않고 직접 모델을 구축해 주세요 !\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm7kPbfsFQ0d"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl_j5EzoFQ0d"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])\n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)\n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djFeShktFQ0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1572f5-c946-4a40-9df4-b13d63a25f0b"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDEhzqZRFQ0e"
      },
      "source": [
        "* 베이스라인 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJE-jG9UFQ0e"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self, in_planes, planes, stride = 1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes,\n",
        "                           kernel_size = 3,\n",
        "                           stride = stride,\n",
        "                           padding = 1,\n",
        "                           bias = False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes,\n",
        "                           kernel_size = 3,\n",
        "                           stride = 1,\n",
        "                           padding = 1,\n",
        "                           bias = False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_planes, planes,\n",
        "                           kernel_size = 3,\n",
        "                           stride = stride,\n",
        "                           padding = 1,\n",
        "                           bias = False),\n",
        "          nn.BatchNorm2d(planes))\n",
        "      \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "class Net(nn.Module):\n",
        "  \n",
        "    def __init__(self): \n",
        "        super(Net, self).__init__() \n",
        "\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16,\n",
        "                                kernel_size = 3,\n",
        "                                stride = 1,\n",
        "                                padding = 1,\n",
        "                                bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(BasicBlock, 16, 2, stride = 1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 32, 2, stride = 2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 64, 2, stride = 2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 128, 2, stride = 2)\n",
        "        self.linear = nn.Linear(128, 21)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "          layers.append(block(self.in_planes, planes, stride))\n",
        "          self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = F.avg_pool2d(x, 8)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model_base = Net().to(DEVICE)\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LowbXUGKFQ0e"
      },
      "source": [
        "* 모델 학습을 위한 함수\n",
        "* 모델 학습, 평가를 위한 가이드라인 코드입니다. 꼭 이 코드를 사용하지는 않으셔도 됩니다 !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GL1mNqzFQ0f"
      },
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()  \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss =  criterion(output, target) # Cross Entropy Loss 사용했습니다\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqvOveaxFQ0f"
      },
      "source": [
        "* 모델 평가를 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncm8J6v4FQ0f"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0 \n",
        "    correct = 0   \n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() \n",
        "   \n",
        "    test_loss /= len(test_loader.dataset) \n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
        "    return test_loss, test_accuracy  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us-wigmEFQ0g"
      },
      "source": [
        "* 모델 학습을 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_hvkStsFQ0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c48d3b6-a215-44cd-badd-6f924a6c934b"
      },
      "source": [
        "import time\n",
        "import copy\n",
        " \n",
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0  \n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()  \n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        \n",
        "        if best_acc <= val_acc: \n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since \n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
        "\n",
        "    model.load_state_dict(best_model_wts)  \n",
        "    return model\n",
        " \n",
        "\n",
        "base = train_baseline(model_base ,train_loader, val_loader, optimizer)  \t\n",
        "torch.save(base,'baseline.pt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 0.0215, Accuracy: 9.63%\n",
            "val Loss: 0.0215, Accuracy: 9.87%\n",
            "Completed in 0m 50s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.0021, Accuracy: 83.44%\n",
            "val Loss: 0.0022, Accuracy: 83.18%\n",
            "Completed in 0m 50s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.0035, Accuracy: 72.42%\n",
            "val Loss: 0.0036, Accuracy: 71.04%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.0015, Accuracy: 86.97%\n",
            "val Loss: 0.0017, Accuracy: 85.09%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.0011, Accuracy: 90.79%\n",
            "val Loss: 0.0013, Accuracy: 88.51%\n",
            "Completed in 0m 50s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.0016, Accuracy: 86.83%\n",
            "val Loss: 0.0018, Accuracy: 85.61%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.0008, Accuracy: 92.96%\n",
            "val Loss: 0.0011, Accuracy: 90.44%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.0009, Accuracy: 92.03%\n",
            "val Loss: 0.0011, Accuracy: 90.65%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.0010, Accuracy: 91.51%\n",
            "val Loss: 0.0012, Accuracy: 89.71%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.0006, Accuracy: 95.13%\n",
            "val Loss: 0.0008, Accuracy: 92.92%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.0003, Accuracy: 97.96%\n",
            "val Loss: 0.0005, Accuracy: 95.40%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.0003, Accuracy: 97.74%\n",
            "val Loss: 0.0005, Accuracy: 95.56%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.0003, Accuracy: 97.40%\n",
            "val Loss: 0.0006, Accuracy: 95.14%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.0007, Accuracy: 95.85%\n",
            "val Loss: 0.0009, Accuracy: 93.58%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.0002, Accuracy: 98.28%\n",
            "val Loss: 0.0004, Accuracy: 96.40%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.0039, Accuracy: 74.84%\n",
            "val Loss: 0.0044, Accuracy: 73.18%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.0053, Accuracy: 71.65%\n",
            "val Loss: 0.0059, Accuracy: 69.34%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.0013, Accuracy: 88.56%\n",
            "val Loss: 0.0017, Accuracy: 85.48%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.0009, Accuracy: 92.26%\n",
            "val Loss: 0.0013, Accuracy: 89.48%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.0001, Accuracy: 98.93%\n",
            "val Loss: 0.0004, Accuracy: 96.50%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 21 ----------------\n",
            "train Loss: 0.0001, Accuracy: 99.10%\n",
            "val Loss: 0.0004, Accuracy: 96.87%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 22 ----------------\n",
            "train Loss: 0.0005, Accuracy: 95.90%\n",
            "val Loss: 0.0008, Accuracy: 93.29%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 23 ----------------\n",
            "train Loss: 0.0012, Accuracy: 89.84%\n",
            "val Loss: 0.0018, Accuracy: 87.05%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 24 ----------------\n",
            "train Loss: 0.0002, Accuracy: 98.64%\n",
            "val Loss: 0.0004, Accuracy: 96.84%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 25 ----------------\n",
            "train Loss: 0.0002, Accuracy: 98.18%\n",
            "val Loss: 0.0005, Accuracy: 96.24%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 26 ----------------\n",
            "train Loss: 0.0000, Accuracy: 99.75%\n",
            "val Loss: 0.0003, Accuracy: 97.55%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 27 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0001, Accuracy: 98.90%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 28 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0002, Accuracy: 98.59%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 29 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0001, Accuracy: 99.06%\n",
            "Completed in 0m 49s\n",
            "-------------- epoch 30 ----------------\n",
            "train Loss: 0.0000, Accuracy: 100.00%\n",
            "val Loss: 0.0001, Accuracy: 99.16%\n",
            "Completed in 0m 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mV-CQwZFQ0h"
      },
      "source": [
        "## 2. Transfer Learning 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDoeutltFQ0h"
      },
      "source": [
        "* Transfer Learning을 위한 준비\n",
        "* Transfer Learning이 익숙하지 않으신 분들은 PyTorch에서 제공하는 https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/transfer_learning_tutorial.html 을 참고하세요 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so4pOWIqFQ0i"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize([64,64]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "    \n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize([64,64]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1SPKCz8MvOT"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_dataset_resnet = ImageFolder(root='./splitted/train', transform = data_transforms['train'])\n",
        "val_dataset_resnet = ImageFolder(root='./splitted/val', transform = data_transforms['val'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRPcQTx1MvOe",
        "outputId": "4ac0dc77-609b-4cea-dd4c-30a37fb85f26"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader_resnet = torch.utils.data.DataLoader(train_dataset_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader_resnet = torch.utils.data.DataLoader(val_dataset_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Tbg8YzFQ0j"
      },
      "source": [
        "### Pre-Trained Model 불러오기\n",
        "- 저는 ResNet50을 사용했는데, 코랩 기준으로 다른 ResNet계열이나 DenseNet 정도까지는 큰 무리 없이 훈련할 수 있습니다. Unfreeze layer 수가 적으면 다른 모델도 사용할 수 있을 것입니다.\n",
        "- 한 가지 모델을 선택해서 Transfer Learning을 해 주세요 !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3CwSwMWFQ0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "c82580168b634ba08ba5f2a541d1f611",
            "0732dc6e1ff84e08a98791647ea17fd1",
            "bc2467c41c5747f390de9276ccf72f0e",
            "58c3ba4fab384101a2ef5075249194ab",
            "f151e33bd5d44613a77ac8a7988e3bc1",
            "33e3fd9ef6bc40fda1668a8c9c7e3f1f",
            "c37c724f97bf4da6a0babc5f6069b9ce",
            "d1e0854929524dc0963a5bf9ea72d4e3"
          ]
        },
        "outputId": "40e2ca54-a3c3-4c18-b48d-c78bd1d4b51d"
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "num_ftrs = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_ftrs, 21)\n",
        "resnet = resnet.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
        "\n",
        "from torch.optim import lr_scheduler  #scheduler는 사용하지 않으셔도 됩니다 (선택사항)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)  #7에폭마다 learning rate를 조절하는 역할을 합니다"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c82580168b634ba08ba5f2a541d1f611",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQoS058cFQ0k"
      },
      "source": [
        "* Pre-Trained Model의 일부 Layer Freeze하기 (resnet 기준입니다 !!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXzgnWxkFQ0k"
      },
      "source": [
        "ct = 0\n",
        "for child in resnet.children():\n",
        "    ct+= 1\n",
        "    if ct < 6:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWdXTYBaFQ0l"
      },
      "source": [
        "* Fine Tuning을 진행해주세요 ~!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q80q_1MYRdZk"
      },
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss =  criterion(output, target) # Cross Entropy Loss 사용했습니다\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKNH31LuRdi2"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "            \n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        " \n",
        "            \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "   \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ9FK199Rdyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5754c4ab-98e3-43e6-b898-b743e2348a81"
      },
      "source": [
        "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        " \n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        since = time.time()\n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        \n",
        "        if best_acc <= val_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since\n",
        "        print('-------------- epoch {} ----------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
        "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "resnet50 = train_baseline(resnet, train_loader_resnet, val_loader_resnet, optimizer_ft)\n",
        "torch.save(resnet50,'resnet50.pt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------- epoch 1 ----------------\n",
            "train Loss: 0.2603, Accuracy: 93.10%\n",
            "val Loss: 0.2946, Accuracy: 92.30%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 2 ----------------\n",
            "train Loss: 0.2105, Accuracy: 94.46%\n",
            "val Loss: 0.2727, Accuracy: 92.82%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 3 ----------------\n",
            "train Loss: 0.1404, Accuracy: 95.61%\n",
            "val Loss: 0.1998, Accuracy: 94.57%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 4 ----------------\n",
            "train Loss: 0.0547, Accuracy: 98.14%\n",
            "val Loss: 0.0971, Accuracy: 97.07%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 5 ----------------\n",
            "train Loss: 0.0468, Accuracy: 98.57%\n",
            "val Loss: 0.0884, Accuracy: 97.41%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 6 ----------------\n",
            "train Loss: 0.0355, Accuracy: 98.98%\n",
            "val Loss: 0.0791, Accuracy: 97.44%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 7 ----------------\n",
            "train Loss: 0.0928, Accuracy: 97.63%\n",
            "val Loss: 0.1806, Accuracy: 95.93%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 8 ----------------\n",
            "train Loss: 0.0199, Accuracy: 99.27%\n",
            "val Loss: 0.0742, Accuracy: 97.68%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 9 ----------------\n",
            "train Loss: 0.0095, Accuracy: 99.76%\n",
            "val Loss: 0.0559, Accuracy: 98.28%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 10 ----------------\n",
            "train Loss: 0.0525, Accuracy: 98.48%\n",
            "val Loss: 0.1166, Accuracy: 97.00%\n",
            "Completed in 0m 60s\n",
            "-------------- epoch 11 ----------------\n",
            "train Loss: 0.0259, Accuracy: 99.10%\n",
            "val Loss: 0.0705, Accuracy: 97.86%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 12 ----------------\n",
            "train Loss: 0.0390, Accuracy: 98.93%\n",
            "val Loss: 0.1153, Accuracy: 97.47%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 13 ----------------\n",
            "train Loss: 0.0129, Accuracy: 99.59%\n",
            "val Loss: 0.0546, Accuracy: 98.59%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 14 ----------------\n",
            "train Loss: 0.0175, Accuracy: 99.50%\n",
            "val Loss: 0.0796, Accuracy: 97.75%\n",
            "Completed in 0m 60s\n",
            "-------------- epoch 15 ----------------\n",
            "train Loss: 0.0239, Accuracy: 99.30%\n",
            "val Loss: 0.0999, Accuracy: 97.49%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 16 ----------------\n",
            "train Loss: 0.0140, Accuracy: 99.57%\n",
            "val Loss: 0.0714, Accuracy: 98.22%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 17 ----------------\n",
            "train Loss: 0.0040, Accuracy: 99.90%\n",
            "val Loss: 0.0416, Accuracy: 98.62%\n",
            "Completed in 1m 0s\n",
            "-------------- epoch 18 ----------------\n",
            "train Loss: 0.0015, Accuracy: 99.97%\n",
            "val Loss: 0.0473, Accuracy: 98.93%\n",
            "Completed in 1m 1s\n",
            "-------------- epoch 19 ----------------\n",
            "train Loss: 0.0298, Accuracy: 99.05%\n",
            "val Loss: 0.1149, Accuracy: 97.21%\n",
            "Completed in 0m 60s\n",
            "-------------- epoch 20 ----------------\n",
            "train Loss: 0.0086, Accuracy: 99.70%\n",
            "val Loss: 0.0680, Accuracy: 98.12%\n",
            "Completed in 0m 60s\n",
            "-------------- epoch 21 ----------------\n",
            "train Loss: 0.0134, Accuracy: 99.57%\n",
            "val Loss: 0.0666, Accuracy: 98.38%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 22 ----------------\n",
            "train Loss: 0.0129, Accuracy: 99.60%\n",
            "val Loss: 0.0709, Accuracy: 98.12%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 23 ----------------\n",
            "train Loss: 0.0116, Accuracy: 99.77%\n",
            "val Loss: 0.0620, Accuracy: 98.09%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 24 ----------------\n",
            "train Loss: 0.0065, Accuracy: 99.85%\n",
            "val Loss: 0.0631, Accuracy: 98.28%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 25 ----------------\n",
            "train Loss: 0.0079, Accuracy: 99.75%\n",
            "val Loss: 0.0794, Accuracy: 98.28%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 26 ----------------\n",
            "train Loss: 0.0061, Accuracy: 99.77%\n",
            "val Loss: 0.0605, Accuracy: 98.33%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 27 ----------------\n",
            "train Loss: 0.0315, Accuracy: 98.99%\n",
            "val Loss: 0.1087, Accuracy: 97.23%\n",
            "Completed in 0m 59s\n",
            "-------------- epoch 28 ----------------\n",
            "train Loss: 0.0139, Accuracy: 99.52%\n",
            "val Loss: 0.0751, Accuracy: 98.02%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 29 ----------------\n",
            "train Loss: 0.0084, Accuracy: 99.77%\n",
            "val Loss: 0.0529, Accuracy: 98.62%\n",
            "Completed in 0m 58s\n",
            "-------------- epoch 30 ----------------\n",
            "train Loss: 0.0240, Accuracy: 99.32%\n",
            "val Loss: 0.0856, Accuracy: 97.94%\n",
            "Completed in 0m 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZXgc0G_FQ0m"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q42vR5_FQ0n"
      },
      "source": [
        "* 모델 평가를 위해서는 평가 데이터 또한 전처리를 해주어야 합니다.\n",
        "* validation 데이터와 동일하게 전처리를 해 주세요 ~!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr5LkMzMRxVk"
      },
      "source": [
        "* 베이스라인 모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EfLf3GOFQ0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74694fb2-77ae-4f0d-ab13-1cb597f45ae5"
      },
      "source": [
        "transform_base = transforms.Compose([transforms.Resize([64,64]), transforms.ToTensor()])\n",
        "test_base = ImageFolder(root='./splitted/test',transform = transform_base)\n",
        "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KISksPR3FQ0n"
      },
      "source": [
        "* Transfer Learning모델 평가를 위한 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BTGBjTzFQ0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9454ba31-3e4e-44ab-d85a-6942df93a5e5"
      },
      "source": [
        "transform_resNet = transforms.Compose([\n",
        "        transforms.Resize([64,64]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet)\n",
        "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J4OJRfCSrrM"
      },
      "source": [
        "### 성능 평가하기\n",
        "* 저는 여기서 accuracy만을 평가했지만, 분류 모델이기에 다양한 방법의 평가가 가능합니다.\n",
        "* Confusion Matrix를 이용한 비교도 가능하고, 한 작물에 해당하는 클래스가 여러개인 다중 분류에서 F1-score를 계산하는것도 의미가 있을 것입니다. \n",
        "* 다양한 시도를 하시는 분께 가산점 드리겠습니다 :):)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGFNIsYfFQ0o"
      },
      "source": [
        "* 베이스라인 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsJCTvboFQ0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7b93e2-613b-4329-b5f3-1946ebefa3a5"
      },
      "source": [
        "baseline=torch.load('baseline.pt')\n",
        "baseline.eval()\n",
        "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
        "\n",
        "print('test acc:  ', test_accuracy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test acc:   98.563593627579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guqq1ZHiFQ0o"
      },
      "source": [
        "* Transfer Learning 모델 성능 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxfqIEkeFQ0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac1d1c8-c733-4ea6-dbdf-494edd365189"
      },
      "source": [
        "resnet50=torch.load('resnet50.pt')\n",
        "resnet50.eval()\n",
        "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
        "\n",
        "print('test acc:  ', test_accuracy)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test acc:   98.79864194306607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8wZSpB6R7c1"
      },
      "source": [
        "* 두 모델의 성능을 비교 평가하는 설명을 작성해주세요 ~!!\n",
        "\n",
        "확실히 전이학습을 할 경우에 모델이 조금 더 빨리 전역최적점에 도달한다는 것을 알 수 있다. 하지만 학습 데이터가 매우 상이한 특성을 가진다면 이는 달라질 수 있을 것이다. 그러나 현재 데이터를 기준으로는 전이 학습시 빠르게 전역최적점에 도달했다. 또한 베이스라인 모델의 경우 renet16으로 설정했고 전이학습시 사용했던 모델은 resnet50인데 이번 데이터의 경우에는 층을 더 깊게 쌓을 수록 성능이 더 높아진다는 것을 베이스라인과 전이학습시 활용한 모델의 성능의 비교를 통해 알 수 있었다.\n",
        "\n",
        "실제 테스트 데이터의 경우에는 학습 데이터로 처음부터 끝까지 학습시킨 베이스라인 모델보다 전이학습을 활용한 모델이 성능이 더 높았다. 전이학습 모델의 경우 일부 레이어의 가중치를 고정시킨 후 학습하였는데 이는 실제 ResNet50을 학습시킨 모델의 데이터와 현재 주어진 Task의 경우 데이터의 특성이 어느정도 비슷하기 때문인 것으로 생각되며 또한 Data Augmentation을 진행하여 조금 더 Nois에 잘 대응하게 만들었기 때문인 것으로 생각된다.\n",
        "\n",
        "결론은 전이학습을 활용한 모델이 성능이 높았는데 이는 기존 ResNet50을 학습시킨 모델의 데이터와 현재 주어진 Task의 경우 데이터의 특성이 어느정도 비슷하며, Data Augmentation과 정규화를 통해서 Nois에 조금 더 잘 대응하도록 하였으며, 층을 조금 더 깊이 쌓아서 이미지의 특성을 조금 더 잘 찾을 수 있게 하여 성능이 조금 더 높아진 것으로 생각된다."
      ]
    }
  ]
}